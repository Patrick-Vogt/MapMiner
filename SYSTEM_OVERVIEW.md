# ğŸ¯ MapMiner - System Overview

## âœ… What Has Been Built

A complete, production-ready web application for automated business data collection from Google Maps with website enrichment.

## ğŸ—ï¸ System Components

### 1. Backend (Flask + WebSocket)
**Location:** `backend/`

- **`app.py`**: Flask server with Flask-SocketIO for real-time communication
  - REST API endpoints for start/stop/status
  - WebSocket events for live progress updates
  - Background thread management for scraping tasks

- **`scraper_orchestrator.py`**: Two-stage workflow manager
  - Coordinates Maps scraping and Website enrichment
  - Progress tracking and status updates
  - Error handling and logging

### 2. Scrapers (Python + Selenium + BeautifulSoup)

- **`maps_scraper_configurable.py`**: Stage 1 - MapMiner scraper
  - Selenium WebDriver for browser automation
  - Configurable delays for human-like behavior
  - Browser selection (Safari, Chrome, Edge)
  - Edge/Chrome browser selection
  - Real-time CSV writing

- **`website_scraper_configurable.py`**: Stage 2 - Website enrichment
  - Parallel processing with ThreadPoolExecutor
  - Email extraction with regex patterns
  - Owner/manager name extraction (German patterns)
  - Thread-safe CSV updates

### 3. Frontend (React + Vite + TailwindCSS)
**Location:** `frontend/`

#### Components:
- **`App.jsx`**: Main application with WebSocket integration
- **`ConfigurationPanel.jsx`**: Form for all scraping parameters
- **`ProgressDisplay.jsx`**: Real-time progress tracking with animations
- **`StatsDisplay.jsx`**: Live statistics cards
- **`LogDisplay.jsx`**: Color-coded activity logs with auto-scroll

#### Styling:
- TailwindCSS for modern, responsive design
- Custom gradient backgrounds
- Smooth animations and transitions
- Mobile-responsive layout

## ğŸ“Š Data Flow

```
User Input (React UI)
    â†“
REST API (Flask)
    â†“
Orchestrator
    â†“
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚   Stage 1       â”‚    Stage 2       â”‚
â”‚  Maps Scraper   â”‚ Website Scraper  â”‚
â”‚   (Selenium)    â”‚  (Parallel)      â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”´â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
    â†“                    â†“
    â””â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
             â†“
        CSV Output
             â†“
    WebSocket Updates â†’ React UI
```

## ğŸ¨ UI Features

### Configuration Panel
- Search term input
- Cities (comma-separated)
- Entries per city slider
- Output path with file browser
- Advanced settings (collapsible):
  - Delay configurations
  - Headless mode toggle
  - Browser selection
  - Max workers for parallel processing
  - Stage 2 enable/disable

### Real-Time Monitoring
- **Progress Bar**: Visual progress with percentage
- **Statistics Cards**: 
  - Maps scraped
  - Websites processed
  - Emails found
  - Owners found
- **Activity Log**: Color-coded logs (info, success, warning, error)
- **Status Badge**: Current stage indicator

## ğŸ”§ Configuration Options

### Basic Settings
- `search_term`: What to search for (e.g., "Autohaus")
- `cities`: Comma-separated list of cities
- `entries_per_city`: Number of results per city (1-100)
- `output_path`: Full path to CSV file

### Advanced Settings
- `delay_min/max`: General delays (default: 2-5s)
- `scroll_delay_min/max`: Scroll delays (default: 3-7s)
- `click_delay_min/max`: Click delays (default: 3-7s)
- `headless`: Run browser hidden (default: false)
- `use_chrome`: Use Chrome instead of Edge (default: false)
- `max_workers`: Parallel threads for Stage 2 (default: 10)
- `run_stage_2`: Enable website enrichment (default: true)

## ğŸ“ Output Format

CSV file with columns:
```
name, address, phone, website, rating, reviews, email, owner
```

### Data Quality Features
- Incremental saving (data preserved if interrupted)
- No backup files (direct write)
- Duplicate email filtering
- Generic email removal (noreply, support, etc.)
- Mobile.de listings automatically skipped

## ğŸš€ Performance

### Stage 1 (Maps Scraping)
- ~2-5 minutes per city (20 entries)
- Sequential processing
- Configurable delays for rate limiting

### Stage 2 (Website Enrichment)
- ~1-2 seconds per website
- Parallel processing (10 workers default)
- Can be scaled up to 50 workers

### Example Timing
- 5 cities Ã— 20 entries = 100 businesses
- Stage 1: ~15 minutes
- Stage 2: ~3-5 minutes
- **Total: ~20 minutes**

## ğŸ” Security & Best Practices

### Rate Limiting
- Random delays between requests
- Configurable delay ranges
- Human-like behavior simulation

### Error Handling
- Graceful failure handling
- Progress preservation on errors
- Detailed error logging

### Data Privacy
- Local processing only
- No external API calls (except target websites)
- User-controlled data storage

## ğŸŒ Cross-Platform Support

### Mac (Development)
- Native Edge/Chrome support
- Bash startup script (`start.sh`)
- Virtual environment support

### Windows (Production)
- Edge/Chrome support
- Batch startup script (`start.bat`)
- Same Python/Node.js requirements

## ğŸ“¦ Dependencies

### Backend (Python)
```
flask==3.0.0
flask-cors==4.0.0
flask-socketio==5.3.5
python-socketio==5.10.0
selenium==4.16.0
requests==2.31.0
beautifulsoup4==4.12.2
```

### Frontend (Node.js)
```
react==19.2.0
socket.io-client==4.7.2
lucide-react==0.460.0
tailwindcss==3.4.0
vite==7.2.4
```

## ğŸ¯ Key Features

### âœ… User-Friendly
- Beautiful, modern UI
- Real-time feedback
- Clear progress indicators
- Intuitive configuration

### âœ… Powerful
- Two-stage data enrichment
- Parallel processing
- Configurable everything
- Production-ready

### âœ… Reliable
- Incremental saving
- Error recovery
- Progress preservation
- Detailed logging

### âœ… Flexible
- Headless mode
- Browser choice
- Custom delays
- Scalable workers

## ğŸ“š Documentation

- **README.md**: Complete documentation
- **QUICKSTART.md**: 5-minute setup guide
- **SYSTEM_OVERVIEW.md**: This file
- Inline code comments

## ğŸ”„ Workflow

1. User configures parameters in React UI
2. Clicks "Start Scraping"
3. Frontend sends config to Flask backend via REST API
4. Backend starts orchestrator in background thread
5. Orchestrator runs Stage 1 (Maps scraping)
   - Selenium opens browser
   - Searches each city
   - Extracts business info
   - Saves to CSV incrementally
6. Orchestrator runs Stage 2 (Website enrichment)
   - Reads CSV
   - Parallel workers visit websites
   - Extract emails and owners
   - Update CSV thread-safely
7. Real-time updates sent via WebSocket
8. Frontend displays progress, logs, and stats
9. User downloads completed CSV

## ğŸ‰ Result

A fully functional, beautiful, and powerful scraper dashboard that:
- Collects business data from Google Maps
- Enriches with email and owner information
- Provides real-time monitoring
- Saves data incrementally
- Works on Mac and Windows
- Ready for production use

## ğŸš¦ Next Steps

1. Install backend dependencies: `cd backend && pip install -r requirements.txt`
2. Frontend dependencies already installed âœ…
3. Run `./start.sh` (Mac) or `start.bat` (Windows)
4. Open http://localhost:5173
5. Configure and start scraping!
